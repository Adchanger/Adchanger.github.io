{"posts":[{"title":"A Unified Model for Multi-class Anomaly Detection","content":" NeurIPS 2022 Motivations 异常类型具有高度多样性，现有方法大多都是对不同类型的对象训练单独的模型，当类的数量较大时实现较为困难； 构建一个模型捕获所有类的特点，使他们具有相同的边界至关重要（b），如果仅关注一个类别，其他类别都将被视为异常； 由于训练的目标是使 loss 尽可能小，因此很多基于重构的异常检测方法的训练结果会倾向于将输入直接作为输出，即：identity shortcut 快捷方式 Contributions 提出了一个 layer-wise query decoder，加强 query 的使用； 认为 full attention 会导致 shortcut 的出现，因此使用 neighbor masked attention模块，使特征点与自己和邻居无关； feature jittering（特征抖动）策略，添加抖动，提高模型鲁棒性； Related Work Anomaly detection Reconstruction detection Transformer in anomaly detection Shortcut Fully-connected layer in MLP y=x+ω+by = x^{+}\\omega + b y=x+ω+b MSE loss倾向于将y逼近x+，进而ω-&gt;I，b-&gt;0； 2. Convolutional layer in CNN 3. Transformer with query embedding y=softmax(q(x+)T/C)x+y = softmax(q(x^{+})^{T} /\\sqrt{C})x^{+} y=softmax(q(x+)T/C​)x+ y-&gt;x时，需要使: softmax(q(x+)T/C)softmax(q(x^{+})^{T} /\\sqrt{C})softmax(q(x+)T/C​)-&gt;I，那么q需要与x强相关，但是q来自正常样本，因此，y更难成为x的shortcut； 虽然Transformer仍然存在shortcut的问题，但它说明了query embedding对改善shortcut是有作用的。 “identical shortcut”and the unified case unifed放大了shortcut问题 Model Neighbor masked encoder（NME） Neighbor masked attention transform的encoder采取full attention策略，每个token可以看到自己和邻居的信息，这使得encoder倾向于复制自己 Neighbor masked attention就是在计算attention时，将自己和邻居节点mask 整体上是Transformer的标准架构，NMA替换Multi-head attention（NMA可能多头的，文中没说？） Layer-wise query decoder（LQD） Transformer中只使用了一次query embedding，LQD想要加强query embedding的使用，因此在每一层都使用一次query embedding Feature jittering 对feature tokens 添加扰动，以提高模型的抗噪能力 Implementation details 特征提取 使用在ImageNet上与训练的EfficientNet-b4作为特征提取器，提取feature map：forgf_{org}forg​ 特征重构 将 forgf_{org}forg​标记为H×W的feature tokens：CorgC_{org}Corg​，然后将 CorgC_{org}Corg​线性投影到更小的通道C，对C进行重构，得到重构的CorgC_{org}Corg​，reshape后得到重构的feature map:frecf_{rec}frec​; 目标函数 L=1H×W∣∣forg−frec∣∣22L = \\frac{1}{H×W}||f_{org}- f_{rec}||^{2}_{2} L=H×W1​∣∣forg​−frec​∣∣22​ 异常定位 对feature map的每个像素点计算异常得分（重构误差的L2范数） S=∣∣forg−frec∣∣2S = ||f_{org}- f_{rec}||_{2} S=∣∣forg​−frec​∣∣2​ 异常检测推断 目的是检测一个图中是否存在异常，图中像素点异常得分的最大值作为该图像的异常得分。 Experiment Anomaly detection on MVTec-AD Anomaly localization on MVTec-AD Anomaly detection on CIFAR-10 Comparison with transformer-based competitors Ablation studies 证明了layer_wise query 、Neighbor masked attention、Feature jittering的有效性 ","link":"https://adchanger.github.io/post/a-unified-model-for-multi-class-anomaly-detection/"},{"title":"Auto-Encoding Variational Bayes.","content":"发 ","link":"https://adchanger.github.io/post/auto-encoding-variational-bayes/"}]}